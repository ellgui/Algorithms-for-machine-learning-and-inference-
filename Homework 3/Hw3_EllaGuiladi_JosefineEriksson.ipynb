{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ledX1hQo5kXC"
   },
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 232 Machine Learning: Home Assignment 3 -- Classification (20 points)** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: K-Nearest-Neighbour (Y), Naive-bayes Classifier (D), Support Vector Machine (D), Logistic Regression (Y)**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Yuchong (Y), Divya (D)** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 6th May** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Ella Guiladi, 930509-0822, guiladi@student.chalmers.se and Josefine Eriksson, 961207-0962, joseerik@student.chalmers.se**<br />\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   **All the answers for theoretical questions must be filled in the cells created for you with \"Your answer here\" below each question, but feel free to add more cells if needed.**\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbqQD9bj6HzP"
   },
   "source": [
    "\n",
    "# Theoretical Questions\n",
    "## 1. K-Nearest-Neighbour Classification (4 pts)\n",
    "### 1.1 Exercise 1 (2 pts)\n",
    "A KNN classifier assigns a test instance the majority class associated with its K nearest training instances. Distance between instances is measured using Euclidean distance. Suppose we have the following training set of positive (+) and negative (-) instances and a single test instance (o). All instances are projected onto a vector space of two real-valued features (X and Y). Answer the following questions. Assume “unweighted” KNN (every nearest neighbor contributes equally to the final vote).\n",
    "\n",
    "![替代文字](https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/knn2.png)\n",
    "\n",
    "a) What would be the class assigned to this test instance for K=1, K=5, K=7 and why? (**1 pt**)\n",
    "\n",
    "b) The classification result is affected by the increasing K, so what will be the maxinum value of K you think in this case? Why? (**1 pt**)\n",
    "(**Hint: After K reaches a certain value, the classification result will not change. Find the value!**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0xnH14a0rsj"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "**a)** Since there is no obvious distance measures on the axis, the assumptions on the distances are estimated visually.\n",
    "\n",
    "K=1 implies classifification through the closest neighbor. For this test instance, the closest neighbor belong to the negative instance, thereby resulting in the test instance being classified as negative. \n",
    "\n",
    "K=5 implies classifification through the 5 closest neighbors. Since the 5 closest neighbors are 3 negative and 2 positive instances, the classification will be determined thorugh a majority vote, i.e the test instance will be classified as negative.\n",
    "\n",
    "K=7 implies classifification through the 7 closest neighbors. The same majorit vote argument is done here as for K=5. However for K=7, the majority class is positive (3 negative and 4 positive instances). This results in the test instance being classified as positive. \n",
    "\n",
    "**b)** Since the positive instances are a majority, the test instance will always be classified as positive after a threshold value of K. Since we have 6 negative instances at a close distance to the test instance, the threshold must be $6*2+1=13$, i.e when K$\\geq 13$, the positive instances will always win the majority vote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqgZRVS80xr7"
   },
   "source": [
    "### 1.2 Exercise 2 (2 pts)\n",
    "Consider 5 data points:\n",
    "\n",
    "$$\\{({0},{1}), ({-1},{0})\\}∈ Class1,$$ \n",
    "\n",
    "$$\\{({1},{0}), ({0},{-1}), (-\\frac{1}{2}, \\frac{1}{2})\\}∈ Class2.$$\n",
    "\n",
    "Consider two test data points:\n",
    "\n",
    "$$(-\\frac{3}{4}, \\frac{3}{4})∈ Class1, (\\frac{1}{2}, \\frac{1}{2})∈ Class2$$\n",
    "\n",
    "Compute the **probability of error** based on k-nearest neighbor rule when $ K=\\{1, 2, 3, 4, 5\\}$ and explain why.\n",
    "(**Hint: The probability of error is the probability of one point is misclassified times the probability of another point is also misclassified**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK-7HuAB0ztS"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "The posterior probability of classifying correctly can be defined through Bayes rule as,\n",
    "\n",
    "$P(C_i|x)=\\dfrac{P(x|C_i)P(C_i)}{P(x)}$,\n",
    "\n",
    "where $P(x|C_i)=\\dfrac{K_i}{N_i V}$, $P(x)=\\dfrac{K}{NV}$ and $P(C_i)=\\dfrac{N_i}{N}$,\n",
    "\n",
    "where V is the volume of the sphere containing K points, K is the number neighbors and $K_i$ is the number of data points from class i within the sphere of volume V. N is the total amount of points such that $\\sum_{i=1}N_i=N$ and $\\sum_{j=1}N_j=N$ for the two test points. \n",
    "\n",
    "Instering these expressions in the posterior probability of classification results in the final expression,\n",
    "\n",
    "$P(C_i|x)=\\dfrac{K_i}{K}$.\n",
    "\n",
    "Thereby, the probability of missclassification (i.e the probability of error) for one test data point is,\n",
    "\n",
    "$P(e_i)=1-P(C_i|x)=1-\\dfrac{K_i}{K}$, where $P(e_i)\\in[0,1]$,\n",
    "\n",
    "so the pobability of error for two test data points is given by,\n",
    "\n",
    "$P(e)=(1-\\dfrac{K_i}{K})(1-\\dfrac{K_j}{K})$, where $P(e)\\in[0,1]$.\n",
    "\n",
    "For K=1 and for the test point belonging to class 1, the following expression is obtained,\n",
    "\n",
    "$P(e_i)=1-\\dfrac{0}{1}=1$, \n",
    "\n",
    "since the nearest neighboring point $(-\\frac{1}{2}, \\frac{1}{2})$ belongs to class 2 and the correct classification of the first test point should be class 1, the probability of missclassification is equal to 1, which is a reasonable result.\n",
    "\n",
    "For K=1 and for the second test point, the two closest points $(1,0) \\in$ class 2 and $(0,1)\\in$ class 1, have the same distance to the test point. The closest neighbor for classification will thereby be chosen by chance, i.e there is 50% chance of missclassification since both cases have equal probability. \n",
    "\n",
    "The probability of error for K=1 thereby becomes,\n",
    "\n",
    "$P(e)=1\\cdot 0.5=0.5$\n",
    "\n",
    "For K=2 and the first test point, the two closest data points are $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2 and $(0,1) \\in$ class 1 or $(-1,0)\\in$ class 1 (equal distance to both points $(0,1) $ and $(-1,0)$). The probability of error for the first test point becomes,\n",
    "\n",
    "$P(e_i)=1-\\dfrac{1}{2}=\\dfrac{1}{2}$\n",
    "\n",
    "For K=2 and the second test point, the two closest data points are $(1,0) \\in$ class 2 and $(0,1)\\in$ class 1, resulting in the probability of error for the second test point to be,\n",
    "\n",
    "$P(e_j)=1-\\dfrac{1}{2}=\\dfrac{1}{2}$.\n",
    "\n",
    "The probability of error for K=2 becomes,\n",
    "\n",
    "$P(e)=\\dfrac{1}{2}\\cdot \\dfrac{1}{2}=\\dfrac{1}{4}$.\n",
    "\n",
    "For K=3 and the first test point, the three closest data points are $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2, $(0,1) \\in$ class 1 and $(-1,0) \\in$ class 1. The probability of error for the first test point becomes,\n",
    "\n",
    "$P(e_i)=1-\\dfrac{2}{3}=\\dfrac{1}{3}$\n",
    "\n",
    "For K=3 and the second test point, the three closest data points are $(1,0) \\in$ class 2, $(0,1)\\in$ class 1 and $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2, resulting in the probability of error for the second test point to be,\n",
    "\n",
    "$P(e_j)=1-\\dfrac{2}{3}=\\dfrac{1}{3}$.\n",
    "\n",
    "The probability of error for K=3 becomes,\n",
    "\n",
    "$P(e)=\\dfrac{1}{3}\\cdot \\dfrac{1}{3}=\\dfrac{1}{9}$.\n",
    "\n",
    "For K=4 and the first test point, the four closest data points are $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2, $(0,1) \\in$ class 1, $(-1,0) \\in$ class 1 and $(0,-1) \\in$ class 2 or $(1,0) \\in$ class 2. The two points $(0,-1) \\in$ class 2 and $(1,0) \\in$ class 2 have the same distance to the test point, thereby there is a 50% chance of choosing either of them, however since both belong to class 2 the choice will not affect the probability of missclassification. The probability of error for the first test point becomes,\n",
    "\n",
    "$P(e_i)=1-\\dfrac{2}{4}=\\dfrac{1}{2}$\n",
    "\n",
    "\n",
    "For K=4 and the second test point, the four closest data points are $(1,0) \\in$ class 2, $(0,1)\\in$ class 1, $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2 and $(0,-1) \\in$ class 2 or $(-1,0)\\in$ class 1. The two points $(0,-1) \\in$ class 2 or $(-1,0 \\in)$ class 1 have the same distance to the test point, thereby there is a 50% chance of choosing either of them. This results in the probability of error for the second test point to be,\n",
    "\n",
    "$P(e_j)=0.5(1-\\dfrac{3}{4})+0.5(1-\\dfrac{2}{4})=\\dfrac{3}{8}$.\n",
    "\n",
    "The probability of error for K=4 becomes,\n",
    "\n",
    "$P(e)=\\dfrac{1}{2}\\cdot \\dfrac{3}{8}=\\dfrac{3}{16}$.\n",
    "\n",
    "For K=5 and the first test point, the five closest data points are $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2, $(0,1) \\in$ class 1, $(-1,0) \\in$ class 1, $(0,-1) \\in$ class 2 and $(1,0) \\in$ class 2. The probability of error for the first test point becomes,\n",
    "\n",
    "$P(e_i)=1-\\dfrac{2}{5}=\\dfrac{3}{5}$\n",
    "\n",
    "For K=5 and the second test point, the five closest data points are $(1,0) \\in$ class 2, $(0,1)\\in$ class 1, $(-\\frac{1}{2}, \\frac{1}{2})\\in$ class 2, $(0,-1) \\in$ class 2 and $(-1,0)\\in$ class 1. This results in the probability of error for the second test point to be,\n",
    "\n",
    "$P(e_j)=1-\\dfrac{3}{5}=\\dfrac{2}{5}$\n",
    "\n",
    "The probability of error for K=5 becomes,\n",
    "\n",
    "$P(e)=\\dfrac{3}{5}\\cdot \\dfrac{2}{5}=\\dfrac{6}{25}=0.24$.\n",
    "\n",
    "It can be observed from the exercise that K should be chosen as an odd intereger in order to avoid the randomness in calculations due to a tie in the majority vote when two data points have the exact same distance to a test point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t9u_kDpgTD2"
   },
   "source": [
    "## 2. [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "### Exercise 2.1 (3 pts)\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 0) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 1),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''? **(1 pt)**\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'') **(2 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEEeDnaN1Ikp"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "**1.** The Naive-Bayes classifier is based on Bayes classifier with the following formula,\n",
    "\n",
    "$P(t_{new}=k|x_{new},X,t)=\\dfrac{P(x_{new}|t_{new}=k,X,t)P(t_{new}=k)}{\\sum_{j=1}P(x_{new}|t_{new}=j,X,t)P(t_{new}=j)}$, \n",
    "\n",
    "\n",
    "The naive assumptions for the Naive-Bayes classifier is given by,\n",
    "\n",
    "1. Independence among features, i.e $P(A,B) = P(A)P(B)$\n",
    "2. Equal influence of the features, i.e the features are equally weighted.\n",
    "\n",
    "\n",
    "Naive-Bayes makes the following additional likelihood assumption:\n",
    "\n",
    "$ P(x_{new}|t_{new}=k,X,t)=\\prod_{d=1}^{D}P(x_{d}^{new}|t_{new}=k,X,t)$.\n",
    "\n",
    "We will use Gaussian class-conditional distributions with the Naive-Bayes assumption and assume that we have a uniform prior, i.e $P(t_{new}=k)=\\dfrac{1}{K}$ so that,\n",
    "\n",
    "$P(t_{new}=1)=\\dfrac{1}{2}$.\n",
    "\n",
    "Since the predicted values either assumes 1 or 0, we assume a binomial distribution for the class conditional densities,\n",
    "\n",
    "$P(x_{new}|t_{new}=k,X,t)=\\prod_{i=1}^{n}P_{k_i}^{x_i}(1-P_{k_i})^{1-x_i}$.\n",
    "\n",
    "We are looking for the probability that a \"content\" person is ''not rich'', ''married'' and ''healthy'', i.e the response,\n",
    "\n",
    "\n",
    "$c = 1: x^*=(0,1,1)$, which can be formulated by the following expression,\n",
    "\n",
    "$P(c=1|x^* )=\\dfrac{P(x^*|c=1)P(c=1)}{P(x^*|c=1)P(c=1)+P(\\bar{x^*}|c=0)P(c=0)}$.\n",
    "\n",
    "The probabilities for each case can be found from the following table,\n",
    "\n",
    "|         | 1 |   |   |   | prob. | 0 |   |   |   | prob. |\n",
    "|---------|---|---|---|---|-------|---|---|---|---|-------|\n",
    "| Rich    | 1 | 0 | 1 | 1 | 3/4   | 0 | 1 | 0 | 0 | 1/4   |\n",
    "| Married | 1 | 0 | 1 | 0 | 1/2   | 0 | 0 | 0 | 1 | 1/4   |\n",
    "| Healthy   | 1 | 1 | 0 | 0 | 1/2   | 0 | 1 | 1 | 0 | 1/2   |\n",
    "\n",
    "Inserteing the probabilities, we obtain the probability to be,\n",
    "\n",
    "$P(c=1|x^* )= \\dfrac{(1-0.75)0.5\\cdot0.5\\cdot0.5}{(1-0.75)0.5\\cdot0.5\\cdot0.5 + (1-0.25)0.25\\cdot0.5\\cdot0.5} = \\dfrac{2}{5}=40\\%.$\n",
    "\n",
    "\n",
    "**2.** The probability that a \"content\" person is ''not rich'' and ''married'' can be obtained by the asumption that the features are independent. This is simply done by leaving out the \"healthy\" data, i.e using the following table,\n",
    "\n",
    "|         | 1 |   |   |   | prob. | 0 |   |   |   | prob. |\n",
    "|---------|---|---|---|---|-------|---|---|---|---|-------|\n",
    "| Rich    | 1 | 0 | 1 | 1 | 3/4   | 0 | 1 | 0 | 0 | 1/4   |\n",
    "| Married | 1 | 0 | 1 | 0 | 1/2   | 0 | 0 | 0 | 1 | 1/4   |\n",
    "\n",
    "with the priors left the same.\n",
    "\n",
    "The Naive-Bayes classifier is calculated the same way as in previous exercise but with the available independent features. We are now looking for the response,\n",
    "\n",
    "$c = 1: x^*=(0,1)$, which can be formulated by the following expression,\n",
    "\n",
    "$P(c=1|x^* )= \\dfrac{(1-0.75)\\cdot0.5\\cdot0.5}{(1-0.75)\\cdot0.5\\cdot0.5 + (1-0.25)0.25\\cdot0.5} = \\dfrac{2}{5}=40\\%.$\n",
    "\n",
    "The result is the same as in precious exercise, which is explained by the fact that the features belonging to \"healthy\" have equal probability of belonging to c=1 as to c=0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FkqFQkN1LWx"
   },
   "source": [
    "### Exercise  2.2 (3 pts)\n",
    "Naive Bayes refers to the classifier which we now describe. We consider here **binary** classification problem with **real valued data** i.e. $x \\in \\mathbb{R}^2$.\n",
    "#### 1. (1 pt)\n",
    "Assume that the class conditional density is **spherical** Gaussian, that is, the likelihood of the training(and testing) data $X, y$ given class $i$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new}, X, y) = P(x_{new} | \\tag{1}\n",
    "\\mu_{i}, \\Sigma_{i})\n",
    "$$\n",
    "\n",
    "Assume both classes have equal prior $p(y= \\pm 1) = 0.5$. Write the expression for the **naive Bayes** classifier, that is, derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\ \\tag{2}\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "***Hint***: Derive the expressions of MLE for parameters in terms of training-data. Then express eq.1 in terms of those estimates using Bayes rule. \n",
    "\n",
    "#### 2. (2 pts)\n",
    "Derive the MLE expression for parameters when the covariance matrix is not diagonal, i.e, Covariance matrix has 4 unknown scalars. This is done to alleviate \"naive\" assumption, since now feature components are no longer independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyg8jlaj1Nb8"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "**1**\n",
    "\n",
    "It is given that the class conditional density is spherical gaussian, which likelihood can be expressed as\n",
    "\n",
    "$$P(x_{new},|y_{new}= \\pm 1,X,y)=P(x_{new},|\\mu^{(\\pm)}, \\Sigma^{(\\pm)}),$$\n",
    "\n",
    "where \n",
    "\n",
    "$P(x_{new},|\\mu^{(\\pm)}, \\Sigma^{(\\pm)}) =\\frac{1}{(2\\pi)^{1/2}|\\Sigma^{(\\pm)}|^{1/2}}\\exp{\\frac{1}{2}(x_{new}-\\mu^{(\\pm)})^T(\\Sigma^{(\\pm)})^{-1}(x_{new}-\\mu^{(\\pm)})} $\n",
    "\n",
    " \n",
    "To find the expressions for the MLE of $\\mu$ and $\\Sigma$ one takes the log of the likelihood\n",
    "\n",
    "$$\\log(P(x_{new},|\\mu^{(\\pm)}, \\Sigma^{(\\pm)}))=\\sum_{n=1}^{N}\\left(-\\frac{1}{2} \\log (2 \\pi)-\\frac{1}{2} \\log \\left(\\mid2\\Sigma^{(\\pm)} \\mid\\right)-\\frac{1}{4 \\Sigma^{(\\pm)}}\\left(\\mathbf{x}_{n}-\\mu^{(\\pm)}\\right)^{T}\\left(\\mathbf{x}_{n}-\\mu^{(\\pm)}\\right)\\right)$$\n",
    "\n",
    "and then the derivative with respect to $\\mu^{(\\pm)}$ and $\\Sigma^{(\\pm)}$ respectively. To find the maximum, one sets the derivatives to zero and searches for the corresponding optimal parameter values. \n",
    "\n",
    "For $\\mu^{(\\pm)}$, this is\n",
    "\n",
    "$$\\frac{\\partial \\log (L)}{\\partial \\boldsymbol{\\mu^{(\\pm)}}}=\\frac{1}{4 \\Sigma^{(\\pm)}} \\sum_{n=1} 2\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(\\pm)}}\\right)=N \\boldsymbol{\\mu^{(\\pm)}}-\\sum_{n=1}^{N} \\mathbf{x}_{n}=0$$ \n",
    "\n",
    "which results in\n",
    "\n",
    "$$\\hat{\\mu}^{(\\pm)} =\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n}.$$\n",
    "\n",
    "The same was done for $\\Sigma^{(\\pm)}=I(\\sigma^{(\\pm)})^2$ as\n",
    "\n",
    "$$\\frac{\\partial \\log (L)}{\\partial \\sigma^{(\\pm)}}=-\\frac{N}{\\sigma^{(\\pm)}}+\\frac{1}{2 (\\sigma^{(\\pm)}})^{3}\\sum_{n=1}^{N}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(\\pm)}}\\right)^{T}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(\\pm)}}\\right)=0$$\n",
    "\n",
    "resulting in\n",
    "\n",
    "$$\\Sigma^{(\\pm)}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(\\pm)}}\\right)^{T}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(\\pm)}}\\right).$$\n",
    "\n",
    "These expressions can now be expressed for this specific trainingdata as, \n",
    "\n",
    "$$\\hat{\\mu}^{(-)} =\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n}\\delta({\\mathbf{y_{n}=-1}})$$\n",
    "$$\\hat{\\mu}^{(+)} =\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n}\\delta({\\mathbf{y_{n}=+1}})$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\\hat{\\Sigma}^{(-)}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(-)}}\\right)^{T}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(-)}}\\right)\\delta({\\mathbf{y_{n}=-1}})$$\n",
    "$$\\hat{\\Sigma}^{(+)}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(+)}}\\right)^{T}\\left(\\mathbf{x}_{n}-\\boldsymbol{\\mu^{(+)}}\\right)\\delta({\\mathbf{y_{n}=+1}})$$\n",
    "\n",
    "The dirac function is used in order to include the right elements from the inputdata.\n",
    "\n",
    "The expressions for the Naive Bayes classifiers becomes:\n",
    "\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) = \\frac{P(x_{new},|\\hat{\\mu}^{(-)}, \\hat{\\Sigma}^{(-)})P(y_{new} = -1)}{P(x_{new},|\\hat{\\mu}^{(+)}, \\hat{\\Sigma}^{(+)})P(y_{new} = +1)+P(x_{new},|\\hat{\\mu}^{(-)}, \\hat{\\Sigma}^{(-)})P(y_{new} = -1)}$$\n",
    "\n",
    "$$P(y_{new} = +1 | x_{new} , X, y ) = \\frac{P(x_{new},|\\hat{\\mu}^{(+)}, \\hat{\\Sigma}^{(+)})P(y_{new} = +1)}{P(x_{new},|\\hat{\\mu}^{(-)}, \\hat{\\Sigma}^{(-)})P(y_{new} = -1)+P(x_{new},|\\hat{\\mu}^{(+)}, \\hat{\\Sigma}^{(+)})P(y_{new} = +1)}.\n",
    "$$\n",
    "\n",
    "with $P(y_{new} = -1)=P(y_{new} = +1)=0.5$ and the MLE expressions derived above.\n",
    "\n",
    "\n",
    "**2**\n",
    "\n",
    "The likelihood for a multivariate gaussian distribution with full covariance can be expressed as:\n",
    "\n",
    "$P(x_{new},|\\mathbf{\\mu}^{i}, \\mathbf{\\Sigma}^{i}) =\\frac{1}{(2\\pi)^{1/2}| \\mathbf{\\Sigma}^{i}|^{1/2}}\\exp{\\frac{1}{2}(x_{new}-\\mathbf{\\mu}^{i})^T( \\mathbf{\\Sigma}^{i})^{-1}(x_{new}-\\mathbf{\\mu}^{i})},$ with $i= +1/-1$ denoting the respective class.  \n",
    "\n",
    "As was done in the derivation of the MLE expressions above, the likelihood function is logaritmized \n",
    "$$\\log(P(x_{new},|\\mathbf{\\mu}^{i}, \\mathbf{\\Sigma}^{i}))=\\sum_{n=1}^{N}\\left(-\\frac{1}{2} \\log (2 \\pi)-\\frac{1}{2} \\log \\left(\\mid2\\mathbf{\\Sigma}^{i} \\mid\\right)-\\frac{1}{4 \\mathbf{\\Sigma}^{i}}\\left(\\mathbf{x}_{n}-\\mathbf{\\mu}^{i}\\right)^{T}\\left(\\mathbf{x}_{n}-\\mathbf{\\mu}^{i}\\right)\\right).$$\n",
    "\n",
    "To get the maximum of the log-likelihood, one takes the derivative of $\\mu$ and $\\Sigma$ respectively and solves for the derivatives equal to zero as above, resulting in \n",
    "\n",
    "$$\\hat{\\mu}_{j}^{i} =\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n,j}\\delta({\\mathbf{y_{n}=i}})$$ with $j=1,...,d$\n",
    "and \n",
    "$$\\hat{\\sigma}_{j,k}^{i}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\mathbf{x}_{n,j}-\\boldsymbol{\\mu_{j}}\\right)^{T}\\left(\\mathbf{x}_{n,k}-\\boldsymbol{\\mu_{k}}\\right)\\delta({\\mathbf{y_{n}=i}})$$ with $j,k=1,...,d$, where d denotes the dimensions of the input data and $i=+1/-1$ denotes the classes. \n",
    "\n",
    "The final expressions corresponding to this dataset is thus\n",
    "$$\\hat{\\mu}_{j}^{+} =\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n,j}\\delta({\\mathbf{y_{n}=+1}}),$$ for j=1,2  \n",
    "$$\\hat{\\mu}_{j}^{-} =\\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{x}_{n,j}\\delta({\\mathbf{y_{n}=-1}}),$$ for j=1,2 \n",
    "and \n",
    "$$\\hat{\\sigma}_{j,k}^{+}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\mathbf{x}_{n,j}-\\boldsymbol{\\mu_{j}^{(+)}}\\right)^{T}\\left(\\mathbf{x}_{n,k}-\\boldsymbol{\\mu_{k}^{(+)}}\\right)\\delta({\\mathbf{y_{n}=+1}}),$$ for j,k=1,2 \n",
    "$$\\hat{\\sigma}_{j,k}^{-}=\\frac{1}{2 N} \\sum_{n=1}^{N}\\left(\\mathbf{x}_{n,j}-\\boldsymbol{\\mu_{j}^{(-)}}\\right)^{T}\\left(\\mathbf{x}_{n,k}-\\boldsymbol{\\mu_{k}^{(-)}}\\right)\\delta({\\mathbf{y_{n}=-1}}),$$ for j,k=1,2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iVQypENwJQ"
   },
   "source": [
    "## 3. [SVM, 5 points]\n",
    "\n",
    "### Excercise 3.1 (2 pts)\n",
    "\n",
    "Consider a (hard margin) SVM with the following training points from\n",
    "two classes:\n",
    "\\begin{eqnarray}\n",
    "+1: &(2,2), (4,4), (4,0) \\nonumber \\\\\n",
    "-1: &(0,0), (2,0), (0,2) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "1. Plot these six training points, and construct, by inspection, the\n",
    "weight vector for the optimal hyperplane. **(1 pt)**\n",
    "\n",
    "2. In your solution, specify the hyperplane in terms of w and b such that $w_1 x_1 + w_2 x_2 + b =0$. Calculate the margin, i.e. $2\\gamma$, where $\\gamma$ is the\n",
    "distance from the hyperplane to its closest data point. (Hint: It may be useful to recall that the distance of a point $(a_1,a_2)$ from the line $w_1x_1 + w_2x_2 + b = 0$ is $|w_1a_1 + w_2a_2 + b|/\\sqrt{w_1^2 + w_2^2}$.) **(1 pt)**\n",
    "\n",
    "### Excercise 3.2 (3 pts)\n",
    "\n",
    "Consider the same problem from above.\n",
    "\n",
    "1. Write the primal formulation of the SVM **for this specific example** i.e. you have to specialise the general formulation for the set of inputs given. **(1 pt)**\n",
    "\n",
    "2. Write the dual formulation **for this specific**. Give the optimal dual solution, comment on support vectors. **(2 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Exercise 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x1=[2,4,4]\n",
    "y1=[2,4,0]\n",
    "x2=[0,2,0]\n",
    "y2=[0,0,2]\n",
    "\n",
    "plt.scatter(x1, y1, marker='+')\n",
    "plt.scatter(x2, y2, marker='_')\n",
    "plt.plot([0,4],[3,-1])\n",
    "plt.title('Training points and the optimal hyperplane')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your answer here:\n",
    "\n",
    "### Excercise 3.1\n",
    "**1**\n",
    "\n",
    "The hyperplane was created with the equation for the straight line by using the information about the training points,\n",
    "\n",
    "$y=-x+3$,\n",
    "\n",
    "and inserting boundary points. \n",
    "\n",
    "By plotting the training points in the plot above it is possible to see that the support vectors for the +1 class are (2,2) and (4,0) and the support vectors for the -1 class are (2,0) and (0,2). We now want to calculate the optimal hyperplane, i.e. the line that optimally separates the two classes. The weight vector for the hyperplane is perpendicular to that hyperplane and thus has the following equation,\n",
    "\n",
    "$W=w_1 x_1+w_2 x_2 + b=0$.\n",
    "\n",
    "In order to calculate this, one can insert the values of the support vectors into $x_1,x_2$, such that for the -1 class,\n",
    "\n",
    "$2w_1+0w_2+b=-1$,\n",
    "\n",
    "$0w_1+2w_2+b=-1$,\n",
    "\n",
    "resulting in,\n",
    "\n",
    "$b=-2w_1-1=-2w_2-1$, i.e $(w_1,w_2)=(1,1)$\n",
    "\n",
    "The same procedure is done for the support vectors from the +1 class, resulting in $(w_1,w_2)=(1,1)$ as well. \n",
    "\n",
    "We can thereby solve for the parameter b by inserting the values of $(w_1,w_2)$,\n",
    "\n",
    "$b=-2w_1-1=-3$.\n",
    "\n",
    "\n",
    "**2**\n",
    "\n",
    "We know that,\n",
    "\n",
    "$2\\gamma=\\dfrac{2|w_1a_1 + w_2a_2 + b|}{\\sqrt{w_1^2 + w_2^2}}$,\n",
    "\n",
    "where $(a_1,a_2)$ is a point close to the hyperplane. We chose $(a_1,a_2)=(2,0)$ (the margin is the same for all the the support vectors), resulting in the following equation,\n",
    "\n",
    "$2\\gamma =\\dfrac{2|1\\cdot2+1\\cdot0-3|}{\\sqrt{1^2+1^2}}=\\sqrt{2}$\n",
    "\n",
    "### Excercise 3.2\n",
    "\n",
    "**1**\n",
    "\n",
    "The primal formulation of the SVM can be expressed as follows: \n",
    "\n",
    "$$argmin_{w} \\frac{1}{2}\\mathbf{w}^T\\mathbf{w},$$\n",
    "\n",
    "subject to\n",
    "\n",
    "$$t_{n}(\\mathbf{w}^{T}\\mathbf{x}_{n}+b)\\geq 1.$$\n",
    "\n",
    "The constraints for this specific example are as follows:\n",
    "\n",
    "$$1\\cdot(2w_1+2w_2+b)\\geq 1$$\n",
    "$$1\\cdot(4w_1+4w_2+b)\\geq 1$$\n",
    "$$1\\cdot(4w_1+0+b)\\geq 1$$\n",
    "\n",
    "$$-1\\cdot(0+0+b)\\geq1$$\n",
    "$$-1\\cdot(2w_1+0+b)\\geq1$$\n",
    "$$-1\\cdot(0+2w_2+b)\\geq1$$\n",
    "\n",
    "**2**\n",
    "\n",
    "The dual formulation of the SVM can be expressed as follows:\n",
    "\n",
    "$$\\underset{\\alpha}{argmax} \\sum_{n=1}^{N}\\alpha_n - \\frac{1}{2}\\sum_{n.m=1}^{N}\\alpha_n\\alpha_m t_n t_m \\mathbf{x}_n^T\\mathbf{x}_m$$\n",
    "\n",
    "subject to\n",
    "$$\\sum_{n=1}^{N}\\alpha_n t_n = 0, \\alpha_n\\geq0,$$\n",
    "\n",
    "where N=6 in our case.\n",
    "\n",
    "In order to get an easier overview of the double sum (the different combinations of $\\alpha_n$ with corresponding class label we can write the matrix,\n",
    "\n",
    "|         | $\\alpha_1$| $\\alpha_2$  | $\\alpha_3$  | $\\alpha_4$  |$\\alpha_5$ | $\\alpha_6$ |\n",
    "|---------|-----------|-------------|-------------|-------------|-----------|------------|\n",
    "| $\\alpha_1$| 8       | 16          | 8           | 0           | -4        | -4         |\n",
    "|$\\alpha_2$ | 16      | 32          | 16          | 0           | -8        | -8         |\n",
    "| $\\alpha_3$| 8       | 16          | 16          | 0           | -8        | 0          |\n",
    "| $\\alpha_4$| 0       | 0           | 0           | 0           | 0         | 0          |\n",
    "| $\\alpha_5$| -4      | -8          | -8          | 0           | 4         | 0          |\n",
    "| $\\alpha_6$| -4      | -8          | 0           | 0           | 0         | 4          |\n",
    "\n",
    "where\n",
    "\n",
    "$[(2,2),t_n = 1,\\alpha_1],$\n",
    "\n",
    "$[(4,4),t_n=1,\\alpha_2],$\n",
    "\n",
    "$[(4,0),t_n=1,\\alpha_3],$\n",
    "\n",
    "$[(0,0)^T,t_n=-1,\\alpha_4].$\n",
    "\n",
    "$[(2,0)^T,t_n=-1,\\alpha_5].$\n",
    "\n",
    "$[(0,2)^T,t_n=-1,\\alpha_6].$\n",
    "\n",
    "By inserting the different combination into the dual formulation presented above, it is possible to get the dual solution, as\n",
    "\n",
    "\n",
    "$$\\underset{\\alpha}{argmax} \\sum_{n=1}^{N}\\alpha_n - (4\\alpha_1^2+8\\alpha_1\\alpha_2+4\\alpha_1\\alpha_3-2\\alpha_1\\alpha_5-2\\alpha_1\\alpha_6+8\\alpha_1\\alpha_2+16\\alpha_2^2+8\\alpha_2\\alpha_3-4\\alpha_2\\alpha_5-4\\alpha_2\\alpha_6+4\\alpha_3\\alpha_1+8\\alpha_3\\alpha_2+8\\alpha_3^2-4\\alpha_3\\alpha_5-2\\alpha_5\\alpha_1-4\\alpha_5\\alpha_2-4\\alpha_5\\alpha_3+2\\alpha_5^2-2\\alpha_6\\alpha_1-4\\alpha_6\\alpha_2+2\\alpha_6^2$$ \n",
    "\n",
    "\n",
    "subject to \n",
    "\n",
    "$$\\Sigma_{n=1}^N \\alpha_n t_n = \\alpha_1 + \\alpha_2 + \\alpha_3 - \\alpha_4 - \\alpha_5 - \\alpha_6 = 0$$\n",
    "\n",
    "$$\\alpha_n \\geq 0$$\n",
    "\n",
    "At the optimum $\\alpha_n\\geq0$ only holds for the points closest to the hyperplane. Thereby the only vectors that will eventually will be used are the support vectors, in order to solve the optimization problem.\n",
    " \n",
    "In order to solve this equation, one can for example use Matlabs quadprog and get the values lagrangian term of the support vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhHF2_-R00si"
   },
   "source": [
    "# Practical Question\n",
    "## 4. Logistic Regression (5 pts)\n",
    "### Customer churn with Logistic Regression\n",
    "A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyse all relevant customer data and develop focused customer retention programs.\n",
    "The dataset includes information about:\n",
    "*   Customers who left within the last month – the column is called Churn.\n",
    "*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.\n",
    "*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
    "*   Demographic info about customers – gender, age range, and if they have partners and dependents.\n",
    "We will help you load and visualise the dataset as well as the preprocessing, you need to build up your logistic regression model step by step and do the prediction.\n",
    "*   **Remember, you are not allowed to use sklearn in modelling and predicting, you have to fill your code in the skeleton.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5cEzzzOVba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "\n",
       "   longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the dataset and read it\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/ChurnData.csv', 'ChurnData.csv')\n",
    "except urllib.error.HTTPError as ex:\n",
    "    print('Problem:', ex)\n",
    "    \n",
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s2gDgP27WNFV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (7, 160) (1, 160)\n",
      "Test set: (7, 40) (1, 40)\n"
     ]
    }
   ],
   "source": [
    "## Data pre-processing and selection\n",
    "## Train/Test dataset split\n",
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df.head()\n",
    "\n",
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(churn_df['churn'])\n",
    "y = np.reshape(y, (np.asarray(churn_df['churn']).shape[0], 1))\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzOgQsCoJSN"
   },
   "source": [
    "**Hints**:\n",
    "- You compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ .\n",
    "- You compute activation $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$.\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$.\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "- You write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n",
    "- In prediction, you calculate $\\hat{Y} = A = \\sigma(w^T X + b)$.\n",
    "- You may use np.exp, np.log(), np.dot(), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCZ4BQ-uW6hg"
   },
   "outputs": [],
   "source": [
    "## Modeling and predicting\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Return: s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1/(1+np.exp(-z)) # calculating the sigmoid function of the input z=w^Tx+b\n",
    "    \n",
    "    return s\n",
    "  \n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized to 0\n",
    "    \"\"\"    \n",
    "    w = np.zeros((dim,1)) # initializing a vector of zeros with dimensions dim * 1\n",
    "    b = 0 # initializing b as zero\n",
    "   \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "  \n",
    "# GRADED FUNCTION: grad_cost\n",
    "def grad_cost(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data of size (number of features, number of examples)\n",
    "    Y -- true \"label\" vector\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\" \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    A =  1/(1+np.exp(-(np.transpose(w).dot(X)+b*np.ones(m)))) # A is a vector with the activation for each observation \n",
    "    cost = -np.mean(Y*np.log(A)+(np.ones(m)-Y)*np.log(np.ones(m)-A))  # calculating the cost using the cost function                        \n",
    "    dw = (1/m)*X.dot(np.transpose(A-Y)) # the derivative of J with respect to w\n",
    "    db = np.mean(A-Y) # the derivative of J with respect to b\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    " # GRADED FUNCTION: optimize\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = grad_cost(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w - learning_rate*dw # updating w using update-rule with alpha=0.001\n",
    "        b = b - learning_rate*db # updating b using update-rule with alpha=0.001\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)     \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}   \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "  \n",
    "# GRADED FUNCTION: predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    A = 1/(1+np.exp(-(np.transpose(w).dot(X)+b*np.ones(m)))) # using activation as prediction of Y\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i]=0;\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "  \n",
    "# GRADED FUNCTION: model\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    Returns: d -- dictionary containing information about the model.\n",
    "    \"\"\"    \n",
    "    # initialize parameters with zeros\n",
    "    w, b = initialize_with_zeros(X_train.shape[0]) # calling the initialization function\n",
    "    \n",
    "    #w, b = initialize_with_zeros(X.shape[1])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False) #calling the optimization function\n",
    "    #parameters, grads, costs = optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    #The train accuracy: The accuracy of a model on examples it was constructed on.\n",
    "    #The test accuracy is the accuracy of a model on examples it hasn't seen.\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "   \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl7F4KSwZo9T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 77.5 %\n",
      "test accuracy: 72.5 %\n"
     ]
    }
   ],
   "source": [
    "## The train accuracy and test accuracy\n",
    "## Feel free to change the hyperparameters\n",
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 20000, learning_rate = 0.003, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
